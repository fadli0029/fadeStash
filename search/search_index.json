{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#experiences","title":"Experiences \ud83d\udcbc","text":"Moss Robotics Inc. Perception Software Engineer Intern (Autonomous Driving) July 2023 - September 2023 Existential Robotics Lab at Contextual Robotics Institute UC San Diego Research Software Engineer January 2023 - current Autonomy Lab at Contextual Robotics Institute UC San Diego Research &amp; Software Engineer Intern March 2022 - September 2022 \u00d7 Moss Robotics Inc. Perception Software Engineer Intern (Autonomous Driving) July 2023 - September 2023         Worked as a software engineer intern working on the robot's perception system for autonomous driving.         <ul> <li>Overcome limitations of single-scan LiDAR data by implementing a point cloud accumulator module.</li> <li>Enhanced trees/plants tracking and detection by implementing fast real-time data association algorithm.</li> <li>Improved tree detection and row-following accuracy with deep learning (YOLO) and parallel line fitting.</li> <li>Automated trees/pots counting through tree block identification by introducing a graph-based approach.</li> <li>Optimized real-time performance and efficiency by leveraging ROS2 Components and using Behavior Trees.</li> <li>Developed all the software entirely in C++.          \u00d7 Existential Robotics Lab at Contextual Robotics Institute UC San Diego Research Software Engineer January 2023 - current         Working on open-source implementations and 3D visualizations of baseline robotics algorithms for localization, mapping, and controls using PyBullet real-time physics simulation. This research project is under Professor Nikolay Atanasov.         <ul> <li>Built implementations &amp; visualizations of mobile robot algorithms for localization, mapping, &amp; controls.</li> <li>Implemented various robotics algorithms like Particle Filter, SLAM, A* search, etc.</li> <li>Programmed a navigation environment in PyBullet real-time physics simulation engine.</li> </ul> \u00d7 Autonomy Lab at Contextual Robotics Institute UC San Diego Research &amp; Software Engineer Intern March 2022 - September 2022         Worked as a software engineering intern and an undergraduate research intern for a Research Project at a Robotics Lab in the Jacobs School of Engineering at UC San Diego. The research was on Vision-Guided Quadrupedal Locomotion under Professor Xiaolong Wang group.         <ul> <li>Deployed Reinforcement Learning policy on Unitree A1 robot allowing it to navigate challenging terrains.</li> <li>Worked with a depth camera (Intel RealSense D435) and other sensors on the robot.</li> <li>Collected real-world data to bridge the gap between Sim2Real and uncertainties in the real world.</li> <li>Utilized GPU clusters and other MLOps tools like Kubernetes and WANDB to train the models.</li> </ul> <p>"},{"location":"#projects","title":"Projects \ud83d\udee0Robust Orientation Tracking for Panoramic Stitching: Projected Gradient Descent vs. Extended Kalman Filters","text":"\u00d7 <p>Abstract</p> <p>Paper</p> <p>The quest for precise 3D orientation tracking of             rotating bodies underpins advancements in robotics, augmented             reality, and navigational systems, necessitating methodologies             that balance accuracy with computational feasibility. This paper             introduces a projected gradient descent (PGD) methodology,             innovatively applied to orientation estimation through sensor             fusion from a 6-DOF inertial measurement unit (IMU). We             undertake a comparative analysis of PGD against established             Extended Kalman Filter (EKF) methods\u2014specifically, 4-state and             7-state variants\u2014anchored by ground truth data from a VICON             motion capture system. Our investigation reveals PGD\u2019s superior             accuracy and robustness over EKF approaches across a spectrum of datasets             characterized by noise, discontinuities, and dynamic changes.             Despite PGD\u2019s reliance on future data, which poses a challenge             for real-time application, its performance advantage is notable,             especially in complex environments. The 7-state EKF, while             outperforming PGD in scenarios with frequent discontinuities,             exhibits limitations in smoothness, highlighting a trade-off             between responsiveness and continuity. The practical utility of             these orientation estimation methods is further demonstrated             through the application of panoramic image stitching, where             PGD\u2019s enhanced performance is evident, although EKF models             provide comparable outcomes under less variable conditions.             This study underscores PGD\u2019s potential as a robust alternative             for 3D orientation tracking, offering insights into its comparative             performance against traditional EKFs. By delineating             the strengths and limitations of PGD and EKF methodologies,             this work contributes to the broader discourse on advancing             sensor-based orientation estimation, encouraging future efforts             to optimize PGD for real-time applications.</p> <p></p> SELF-DRIVING RC CAR  \ud83d\ude97 An RC Car, powered by Jetson Nano and a custom ROS package, capable of driving autnomously, keeping itself on track and change lanes when needed. GAME CONTROLLER  \ud83c\udfae A game controller made with ESP32, accelerometer, OLED display, and more. It's capable of playing both Space Invader and Mine Sweeper game. SMART WEARABLE  \u231a This wearable has multiple features, such as measuring heart rate and steps count, retrieving real-time weather forecast, and showing time &amp; date."},{"location":"Projects/AutonomousRCCar/","title":"Autonomous RC Car   \ud83d\ude97","text":"<p> <p></p> <p></p>"},{"location":"Projects/AutonomousRCCar/#self-driving-rc-car-powered-by-jetson-nano","title":"Self-Driving RC Car Powered by JETSON NANO","text":""},{"location":"Projects/AutonomousRCCar/#featured-on-nvidia-community-jetson-projects-page","title":"\ud83d\udc51  Featured on NVIDIA community Jetson Projects page  \ud83d\udc51","text":"<p>Documentation   \ud83d\udcd1 Mjolnir Kit - GitHub </p>"},{"location":"Projects/AutonomousRCCar/#sowhat-is-this","title":"So..what is this? \ud83e\udd14","text":"<p>Meet Harold     , an RC Car, capable of driving autonomously whilst maintaining itself on the track, and switch lanes when needed.  </p> <p> Harold \ud83d\ude97 </p> <p></p> <p> Harold driving 3 laps in a challenging curvy track, like a boss   \ud83d\ude0e </p>"},{"location":"Projects/AutonomousRCCar/#features","title":"Features   \ud83c\udf1f","text":"<p>Info</p> <p>This is just a very brief write-up of the project. The full, detailed, documentation can be found here </p> <ul> <li>Uses a custom ROS Package which utilizes OpenCV, we call it Mjolnir Kit, allowing it to detect lines and lanes, to stay on the track and to switch lanes when needed. \ud83d\udc49  :fontawesome-brands-github-square: GitHub</li> <li>The brain of the car is the powerful Jetson NANO.</li> <li>It also features a PID Controller App, which allows user to tune the PID of the car, live while it's driving. This is achieved via MQTT protocol and a PID Controller class.</li> <li>A custom PCB board for power distribution and an LED board to add aesthetics to our car.  </li> </ul> <p> a sneak peek of the Mjolnir Kit </p>"},{"location":"Projects/AutonomousRCCar/#official-documentation","title":"Official Documentation   \ud83d\udcd1","text":"<p>The official documentation is currently hosted here, though I am considering to document it on this project page too. So, stay tuned!</p>"},{"location":"Projects/AutonomousRCCar/#meet-harolds-engineers","title":"Meet Harold's Engineers   \ud83d\udc6c\ud83d\udc6c","text":"<p> From left: @George Troulis(Computer Engineering), @Arthur Dassier(Computer Science), @Myself(Electrical Engineering), @Dev Gulati(Mechanical Engineering) </p> <p></p> <p> For full, detailed, documentation, consult our documentation page here. </p>"},{"location":"Projects/GameController/","title":"Game Controller   \ud83c\udfae","text":"<p> <p></p> <p></p>"},{"location":"Projects/GameController/#game-controller-that-plays-space-invader-and-mine-sweeper","title":"Game Controller that plays Space Invader    and Mine Sweeper","text":"<p>GitHub </p>"},{"location":"Projects/GameController/#sowhat-is-this","title":"So..what is this? \ud83e\udd14","text":"<p>I made an immersive Bluetooth-enabled Game Contoller that can play both MineSweeper and Space Invader using ESP32, accelerometer, and a bunch of other electronics.  </p> <p> The controller (note: i plan to 3D printed the case in the near future) </p> <p> Gameplay demo, Space Invader   \ud83d\udc7e  </p> <p> My teammate, Justin, playing the Jumping Jacks Mode </p>"},{"location":"Projects/GameController/#features","title":"Features   \ud83c\udf1f","text":"<p>Info</p> <p>This is just a very brief write-up of the project. The full, detailed, documentation can be found here</p> <ul> <li>Uses ESP32 and accelerometer to detect player's movements and reactions.</li> <li>Live game statistics display on the OLED and vibrations when the player gets hit.</li> <li>An immersive Bluetooth-enabled Game Controller that can play both MineSweeper   \ud83d\udca3 and Space Invader   \ud83d\udc7e .</li> <li>A \"Jumping Jacks Mode\" for the MineSweeper game, where a player is required to perform a number of jumping jacks in order to choose a particular tile. Detection of this jumping jack is achieved via Digital Signal Processing on the Acceleromter Data, particularly Moving Average Filter and Detrending.</li> <li>Simultaneous firing and movement via a push of a button.</li> <li>Demo videos: Space Invader   \ud83d\udc7e | Mine Sweeper   \ud83d\udca3</li> </ul>"},{"location":"Projects/GameController/#official-documentation","title":"Official Documentation   \ud83d\udcd1","text":"<p>The official documentation is currently hosted on GitHub , though I am considering to document it on this project page too. So, stay tuned!</p> <p></p> <p> For full, detailed, documentation, consult the project's GitHub  Repository </p>"},{"location":"Projects/SmartWearable/","title":"Smart Wearable   \u231a","text":"<p> <p></p>"},{"location":"Projects/SmartWearable/#smart-wearable-that-measures-heart-rate-displays-weather-forecast-counts-steps-and-time-date","title":"Smart Wearable That Measures Heart Rate   \ud83d\udc93, Displays Weather Forecast   \u2614, Counts Steps   \ud83d\udeb6, and Time &amp; Date   \ud83d\udd51","text":""},{"location":"Projects/SmartWearable/#sowhat-is-this","title":"So..what is this? \ud83e\udd14","text":"<p>I made a wearable watch that can measure your heart rate, displays weather forecast, measures steps count, and shows time &amp; date. It uses an ESP32, Photodetector, Accelerometer, and an OLED Display.</p> <p> Yeah, you probably seen this before here, but yes it is both a a smart wearable and a game controller   \ud83d\ude0e maybe I should make it look like an actual watch   \ud83e\udd14  </p> <p> Collecting and training heart beat data via Gaussian Mixture Training   \ud83e\udde0  </p> <p> The wearable in action, measuring my heart rate, and displaying all other features </p>"},{"location":"Projects/SmartWearable/#features","title":"Features   \ud83c\udf1f","text":"<ul> <li>Measures live heart rate through data collected by photodetector. Digital Signal Processing is then applied on this data to be trained via Gaussian Mixture Training (GMM), and validated with Leave-One-Subject-Out-Validation (LOSOV) method.</li> <li>Measures steps count via applying Digital Signal Processing on Accelerometer data.</li> <li>Provides Live Weather Forecast and Time &amp; Date display, achieved through OpenWeather Map API.</li> </ul>"}]}